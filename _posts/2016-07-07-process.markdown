---
layout: post
title: Data pipeline
date: '2016-07-07 21:16:15 +0200'
categories: project
published: true
---
The process that the data follows is the one shown in the diagram:

1. The tweets are collected and added to a Kafka queue.
2. The queued tweets are filtered excluding the ones that don't include an URL. The tweets with an URL are added to the "url filter" Kafka queue.
3. From the "url filter" queue, the tweets are filtered by language and then separated in two groups: tweets in Spanish and tweets in English, added to different Kafka queues for each language.
4. Each group are filtered again in order to exclude their duplicates and near duplicates and the added to a new Kafka queue.
5. From that last queue, the features of each tweet are extracted, saving them into a JSON file:
	* Features from the tweets.
	* Features from their URLs, using web scrapping.
6. The extracted features are vectorized.
7. The features are processed by the training model.

<img class="img-responsive pull-right" width="80%" src="/assets/images/process.png" alt="{{ post.title }}"/>
